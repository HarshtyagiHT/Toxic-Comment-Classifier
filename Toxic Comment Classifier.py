# -*- coding: utf-8 -*-
"""ML Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qTwbBoPZh1k0JuaQow6tdIF4SSZ9UaHL
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import re
import nltk
import string
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import stopwords
from timeit import default_timer as timer

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC

from sklearn.metrics import f1_score, recall_score, precision_score
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import cross_val_score
from statistics import mean
from sklearn.metrics import hamming_loss
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import learning_curve

from sklearn.metrics import confusion_matrix
import statistics
from sklearn.metrics import recall_score

from wordcloud import WordCloud
from collections import Counter

from sklearn.pipeline import Pipeline

from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import VotingClassifier
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# %matplotlib inline

data = pd.read_csv('/content/train.csv')
test_label = pd.read_csv('/content/test_labels.csv')

data.head()

data.describe()

test = pd.read_csv('/content/test.csv')

test.head()

test_label.head()

data.shape

test.shape

sns.set(color_codes=True)
comment_length = data.comment_text.str.len()
sns.distplot(comment_length, kde=False, color="steelblue", bins=20)

# Subsetting labels from the training data
training_labels = data[['toxic', 'severe_toxic',
                      'obscene', 'threat', 'insult', 'identity_hate']]
label_count = training_labels.sum()
label_count.plot(kind='bar', title='Labels Frequency', color='steelblue')

# Code to draw bar graph for visualising distribution of classes within each label.
barWidth = 0.25

bar1 = [sum(data['toxic'] == 1), sum(data['obscene'] == 1), sum(data['insult'] == 1), sum(data['severe_toxic'] == 1),
         sum(data['identity_hate'] == 1), sum(data['threat'] == 1)]
bar2 = [sum(data['toxic'] == 0), sum(data['obscene'] == 0), sum(data['insult'] == 0), sum(data['severe_toxic'] == 0),
         sum(data['identity_hate'] == 0), sum(data['threat'] == 0)]

range_1 = np.arange(len(bar1))
range_2 = [x + barWidth for x in range_1]

plt.bar(range_1, bar1, color='steelblue', width=barWidth, label='labeled = 1')
plt.bar(range_2, bar2, color='lightsteelblue', width=barWidth, label='labeled = 0')

plt.xlabel('group', fontweight='bold')
plt.xticks([r + barWidth for r in range(len(bar1))], ['Toxic', 'Obscene', 'Insult', 'Severe Toxic', 'Identity Hate',
                                                       'Threat'])
plt.legend()
plt.show()

data.comment_text[0]

data[data.toxic == 1].iloc[1, 1]
row_sums = data.iloc[:, 2:].sum(axis=1)
temp = data.iloc[:, 2:-1]
train_corr = temp[row_sums > 0]
corr = train_corr.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr,
            xticklabels=corr.columns.values,
            yticklabels=corr.columns.values, annot=True, cmap="Blues")

def Word_Cloud(token):
    """ Visualize the most common words contributing to the token. """
    threat = data[data[token] == 1]
    threat_txt = threat.comment_text
    neg_text = pd.Series(threat_txt).str.cat(sep=' ')
    wc = WordCloud(background_color="white", width=1600, height=800,
                          max_font_size=200).generate(neg_text)

    plt.figure(figsize=(15, 10))
    plt.imshow(wc.recolor(colormap="Accent"), interpolation='bilinear')
    plt.axis("off")
    plt.title(f"Most common words assosiated with {token} comment", size=20)
    plt.show()

# interactive visual : enter the label name.
lbl = input('Choose a class to visualize the most common words contributing to the class:')
Word_Cloud(lbl.lower())

test_labels = ["toxic", "severe_toxic", "obscene",
               "threat", "insult", "identity_hate"]

def tokenize(txt):
    txt = txt.lower()
    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\r\\t\\n]')
    remove_n = regex.sub(" ", txt)
    words = remove_n.split(' ')
    # remove any non ascii
    words = [word.encode('ascii', 'ignore').decode('ascii') for word in words]
    lematzr = WordNetLemmatizer()
    words = [lematzr.lemmatize(w) for w in words]
    words = [w for w in words if len(w) > 2]
    return words

import nltk
nltk.download('wordnet')
vectorizer = TfidfVectorizer(ngram_range=(1, 1), analyzer='word',
                         tokenizer=tokenize, stop_words='english',
                         strip_accents='unicode', use_idf=1, min_df=10)
X_train = vectorizer.fit_transform(data['comment_text'])
X_test = vectorizer.transform(test['comment_text'])

vectorizer.get_feature_names()[0:20]

# Creating classifiers with default parameters initially.
mnb = MultinomialNB()
lr = LogisticRegression()
svm = LinearSVC()

def cross_validation_score(classifier, X_train, y_train):
    ''' Iterate though each label and return the cross validation F1 and Recall score '''
    methods = []
    name = classifier.__class__.__name__.split('.')[-1]

    for label in test_labels:
        recall = cross_val_score(
            classifier, X_train, y_train[label], cv=10, scoring='recall')
        f1_score = cross_val_score(classifier, X_train,
                             y_train[label], cv=10, scoring='f1')
        methods.append([name, label, recall.mean(), f1_score.mean()])

    return methods

# Calculating the cross validation F1 and Recall score for our 3 baseline models.
cv1 = pd.DataFrame(cross_validation_score(mnb, X_train, data))
cv2 = pd.DataFrame(cross_validation_score(lr, X_train, data))
cv3 = pd.DataFrame(cross_validation_score(svm, X_train, data))

# Creating a dataframe to show summary of results.
method_cv = pd.concat([cv1, cv2, cv3])
method_cv.columns = ['Model', 'Label', 'Recall', 'F1']
meth_cv = method_cv.reset_index()
meth_cv[['Model', 'Label', 'Recall', 'F1']]

def score(classifier, X_train, y_train, X_test, y_test):
    """
    Calculate Hamming-loss, F1, Recall for each label on test dataset.
    """
    methods = []
    h_loss = []
    name = classifier.__class__.__name__.split('.')[-1]
    predict_df = pd.DataFrame()
    predict_df['id'] = test_label['id']

    for label in test_labels:
        classifier.fit(X_train, y_train[label])
        predicted = classifier.predict(X_test)

        predict_df[label] = predicted

        recall = recall_score(y_test[y_test[label] != -1][label],
                              predicted[y_test[label] != -1],
                              average="weighted")
        f1 = f1_score(y_test[y_test[label] != -1][label],
                      predicted[y_test[label] != -1],
                      average="weighted")

        conf_mat = confusion_matrix(y_test[y_test[label] != -1][label],
                                    predicted[y_test[label] != -1])

        methods.append([name, label, recall, f1, conf_mat])

    hamming_loss_score = hamming_loss(test_label[test_label['toxic'] != -1].iloc[:, 1:7],
                                      predict_df[test_label['toxic'] != -1].iloc[:, 1:7])
    h_loss.append([name, hamming_loss_score])

    return h_loss, methods

# Calculating the Hamming-loss F1 and Recall score for our 3 baseline models.
h1, method1 = score(clf1, X_train, data, X_test, test_label)
h2, method2 = score(clf2, X_train, data, X_test, test_label)
h3, method3 = score(clf3, X_train, data, X_test, test_label)

# Creating a dataframe to show summary of results.
method1 = pd.DataFrame(method1)
method2 = pd.DataFrame(method2)
method3 = pd.DataFrame(method3)
methods = pd.concat([method1, method2, method3])
methods.columns = ['Model', 'Label', 'Recall', 'F1', 'Confusion_Matrix']
meth = methods.reset_index()
meth[['Model', 'Label', 'Recall', 'F1']]

# Visualizing F1 score results through box-plot.
axn = sns.boxplot(x='Model', y='F1', data=methods, palette="Blues")
sns.stripplot(x='Model', y='F1', data=methods,
              size=8, jitter=True, edgecolor="gray", linewidth=2, palette="Blues")
axn.set_xticklabels(axn.get_xticklabels(), rotation=20)

plt.show()

# Commented out IPython magic to ensure Python compatibility.
# Code to create bar graph of F1 and Recall across each label for Multinomial Naive Bayes
print("Plot for Multinomial Naive Bayes regression")
meth_2 = methods[methods.Model == 'MultinomialNB']

meth_2.set_index(["Label"], inplace=True)
# %matplotlib inline
meth_2.plot(figsize=(16, 8), kind='bar', title='Metrics',
        rot=60, ylim=(0.0, 1), colormap='tab10')

# Commented out IPython magic to ensure Python compatibility.
# Code to create bar graph of F1 and Recall across each label for Logistic regression
print("Plot for Logistic regression")
meth_2 = methods[methods.Model == 'LogisticRegression']

meth_2.set_index(["Label"], inplace=True)
# %matplotlib inline
meth_2.plot(figsize=(16, 8), kind='bar', title='Metrics',
        rot=60, ylim=(0.0, 1), colormap='tab10')

# Commented out IPython magic to ensure Python compatibility.
# Code to create bar graph of F1 and Recall across each label for Linear SVC
print("Plot for Linear SVC")
meth_2 = methods[methods.Model == 'LinearSVC']

meth_2.set_index(["Label"], inplace=True)
# %matplotlib inline
meth_2.plot(figsize=(16, 8), kind='bar', title='Metrics',
        rot=60, ylim=(0.0, 1), colormap='tab10')

def drawConfusionMatrix(cm):
    """
    Plot Confusion matrix of input cm.
    """
    cm = cm.astype('float')/cm.sum(axis=1)[:, np.newaxis]
    axs = plt.axes()
    sns.heatmap(cm,
                annot=True,
                annot_kws={"size": 16},
                cmap="Blues",
                fmt='.2f',
                linewidths=2,
                linecolor='steelblue',
                xticklabels=("Non-toxic", "Toxic"),
                yticklabels=("Non-toxic", "Toxic"))

    plt.ylabel('True', fontsize=18)
    plt.xlabel('Predicted', fontsize=18)
    plt.show()

def Matrix(label):
    """
    Plot Confusion matrix for each label and call function drawConfusionMatrix().
    """
    print(f"-------------- {label} labelling --------------")
    labels = {"toxic": 0, "severe_toxic": 1, "obscene": 2,
              "threat": 3, "insult": 4, "identity_hate": 5}

    pos = labels[label]
    for i in range(pos, len(meth), 6):
        print()
        print(f"----  {meth['Model'][i]}  ----")
        cm = meth['Confusion_Matrix'][i]
        drawConfusionMatrix(cm)

token = input('Choose a class for the Confusion Matrix: ')
Matrix(token.lower())

# Creating a dataframe to summarize Hamming-loss
df1 = pd.DataFrame(h1)
df2 = pd.DataFrame(h2)
df3 = pd.DataFrame(h3)

hammingloss = pd.concat([df1, df2, df3])
hammingloss.columns = ['Model', 'Hamming_Loss']
hl = hammingloss.reset_index()
hl[['Model', 'Hamming_Loss']]

pipe_lr = Pipeline([
    ('lr', LogisticRegression(class_weight="balanced"))
])

pipe_linear_svm = Pipeline([
    ('svm', LinearSVC(class_weight={1: 20}))
])

pipelines = [pipe_lr, pipe_linear_svm]

score_df = []
for pipe in pipelines:
    f1_val = []
    recall_val = []
    hl = []
    train_time = []
    predict_df = pd.DataFrame()
    predict_df['id'] = test_label['id']
    for label in test_labels:
        start = timer()
        pipe.fit(X_train, data[label])
        train_time = timer() - start
        predicted = pipe.predict(X_test)
        predict_df[label] = predicted

        f1_val.append(f1_score(
            test_label[test_label[label] != -1][label], predicted[test_label[label] != -1], average="weighted"))
        recall_val.append(recall_score(
            test_label[test_label[label] != -1][label], predicted[test_label[label] != -1], average="weighted"))
        train_time.append(train_time)
        name = pipe.steps[-1][1].__class__.__name__.split('.')[-1]

    hamming_loss_score = hamming_loss(
        test_label[test_label['toxic'] != -1].iloc[:, 1:7], predict_df[test_label['toxic'] != -1].iloc[:, 1:7])

    val = [name, mean(f1_val), mean(recall_val),
           hamming_loss_score, mean(train_time)]
    score_df.append(val)

score = pd.DataFrame(score_df,)
score.columns = ['Model', 'F1', 'Recall', 'Hamming_Loss', 'Training_Time']
score

lr_classifier = LogisticRegression()

parameter_grid = {'solver': ['newton-cg', 'lbfgs', 'liblinear'],
                  'class_weight': [None, 'balanced']}

cross_validation = StratifiedKFold(n_splits=5)

gridsearch = GridSearchCV(lr_classifier,
                           param_grid=parameter_grid,
                           cv=cross_validation,
                           scoring='f1')

gridsearch.fit(X_train, data['toxic'])

print('Best parameters: {}'.format(gridsearch.best_params_))

gridsearch.best_estimator_

svm_classifier = LinearSVC()

parameter_grid = {'class_weight': [None, 'balanced'],
                  'C': [1, 5, 10]}

cross_validation = StratifiedKFold(n_splits=5)

grid_search = GridSearchCV(svm_classifier,
                           param_grid=parameter_grid,
                           cv=cross_validation,
                           scoring='f1')

grid_search.fit(X_train, data['toxic'])

print('Best parameters: {}'.format(grid_search.best_params_))

grid_search.best_estimator_

svm_clf = LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,
                    intercept_scaling=1, loss='squared_hinge', max_iter=1000,
                    multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
                    verbose=0)

lr_clf = lr_clf = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                                     intercept_scaling=1, max_iter=100, multi_class='multinomial',
                                     n_jobs=None, penalty='l2', random_state=None, solver='newton-cg',
                                     tol=0.0001, verbose=0, warm_start=False)

tunned_model_score_df = []
for model in [svm_clf, lr_clf]:
    f1_val = []
    recall_val = []
    hl = []
    training_time = []
    predict_df = pd.DataFrame()
    predict_df['id'] = test_label['id']

    for label in test_labels:
        start = timer()
        model.fit(X_train, data[label])
        training_time.append(timer() - start)
        predicted = model.predict(X_test)
        predict_df[label] = predicted

        f1_val.append(f1_score(test_label[test_label[label] != -1][label],
                                  predicted[test_label[label] != -1],
                                  average="weighted"))
        recall_val.append(recall_score(test_label[test_label[label] != -1][label],
                                          predicted[test_label[label] != -1],
                                          average="weighted"))
        name = model.__class__.__name__

    hamming_loss_score = hamming_loss(test_label[test_label['toxic'] != -1].iloc[:, 1:7],
                                      predict_df[test_label['toxic'] != -1].iloc[:, 1:7])

    val = [name, mean(f1_val), mean(recall_val),
           hamming_loss_score, sum(training_time)]

    tunned_model_score_df.append(val)

ans = pd.DataFrame(tunned_model_score_df,)
ans.columns = ['Model', 'F1',
                         'Recall', 'Hamming_Loss', 'Traing_Time']
ans

ab_clf = AdaBoostClassifier()
gb_clf = GradientBoostingClassifier()
xgb_clf = xgb.XGBClassifier()
boosting_models = [ab_clf, gb_clf, xgb_clf]
boosting_score_df = []
for model in boosting_models:
    f1_vals = []
    recall_vals = []
    train_time = []
    predicted_df = pd.DataFrame()
    predicted_df['id'] = test_label['id']

    for idx, label in enumerate(test_labels):
        start = timer()
        model.fit(X_train, data[label])
        prediction = model.predict(X_test)
        train_time.append(timer() - start)
        predicted_df[label] = prediction
        f1_vals.append(f1_score(test_label[test_label[label] != -1][label],
                                  prediction[test_label[label] != -1],
                                  average="weighted"))
        recall_vals.append(recall_score(test_label[test_label[label] != -1][label],
                                          prediction[test_label[label] != -1],
                                          average="weighted"))
        name = model.__class__.__name__

    hamming_loss_score = hamming_loss(test_label[test_label['toxic'] != -1].iloc[:, 1:7],
                                      predicted_df[test_label['toxic'] != -1].iloc[:, 1:7])
    val = [name, mean(f1_vals), mean(recall_vals),
           hamming_loss_score, mean(train_time)]

    boosting_score_df.append(val)

boosting_score = pd.DataFrame(boosting_score_df,)
boosting_score.columns = ['Model', 'F1',
                          'Recall', 'Hamming_Loss', 'Traing_Time']
boosting_score

ensemble_classifier = VotingClassifier(estimators=[('lr', lr_clf),
                                            ('svm', svm_clf),
                                            ('xgb', xgb_clf)], voting='hard')
ensemble_score_df = []
f1_vals = []
recall_vals = []
hl = []
train_time = []

predict_df = pd.DataFrame()
predict_df['id'] = test_label['id']
for label in test_labels:
    start = timer()
    ensemble_classifier.fit(X_train, data[label])
    train_time.append(timer() - start)
    predicted = ensemble_classifier.predict(X_test)
    predict_df[label] = predicted
    f1_vals.append(f1_score(test_label[test_label[label] != -1][label],
                              predicted[test_label[label] != -1],
                              average="weighted"))
    recall_vals.append(recall_score(test_label[test_label[label] != -1][label],
                                      predicted[test_label[label] != -1],
                                      average="weighted"))
    name = 'Ensemble'

h_loss_score = hamming_loss(test_label[test_label['toxic'] != -1].iloc[:, 1:7],
                                  predict_df[test_label['toxic'] != -1].iloc[:, 1:7])

val = [name, mean(f1_vals), mean(recall_vals),
       h_loss_score, mean(train_time)]
ensemble_score_df.append(val)


# printing the values
ensemble_score = pd.DataFrame(ensemble_score_df,)
ensemble_score.columns = ['Model', 'F1',
                          'Recall', 'Hamming_Loss', 'Training_Time']
ensemble_score

label = 'toxic'
lr = LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,
               intercept_scaling=1, loss='squared_hinge', max_iter=1000,
               multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
               verbose=0)
lr.fit(X_train, data[label])
Toxic_LR = lr.predict(X_test)
combined_test = pd.concat([test, test_label], axis=1)
Check_comment = combined_test[(combined_test.toxic == 1) & (
    Toxic_LR == 0)].comment_text
Check_comment.shape

# extract wrongly classified comments
Check_comment = combined_test[(combined_test.toxic == 1) & (
    Toxic_LR == 0)].comment_text

Check_negative = pd.Series(Check_comment).str.cat(sep=' ')
wc = WordCloud(width=1600, height=800,
                      max_font_size=200).generate(Check_negative)
plt.figure(figsize=(15, 10))
plt.imshow(wc.recolor(colormap="Blues"), interpolation='bilinear')
plt.axis("off")
plt.title("Most common words from misclassified", size=20)
plt.show()

wrongWords = tokenize(Check_negative)
stop_words = stopwords.words('English')
wrongWords = [w for w in wrongWords if w not in stop_words]
counter = Counter(wrongWords)
counter.most_common(20)

neg_text_train = data['comment_text'].str.cat(sep=' ')
counter_train = Counter(tokenize(neg_text_train))
counter_train.get('ucking')